{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Author: Karen Yin-Yee Ng \n",
      "\n",
      "Stat 250 Winter 2014  Assignment 1 part 1 \n",
      "\n",
      "Written with an ipython notebook (v 1.1.0)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autosave 60"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "IPython.notebook.set_autosave_interval(60000)"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Autosaving every 60 seconds\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. method: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We benchmark different approaches for computing the statistics of airline delays between Jan 1987 through Sept 2012. \n",
      "\n",
      "Design considerations, due to the large size of the data, we have to : \n",
      "\n",
      "*  avoid holding all the data in memory at same time but only want to extract the relevant data\n",
      "\n",
      "* avoid numerical instabilities \n",
      "\n",
      "* make it fast - avoid expensive operations such as making copies of data or I/O to hard disk\n",
      "\n",
      "    * may want to make it parallelizable depending on performance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exact approach 1 : frequency table approach using the shell & R "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pros: \n",
      "\n",
      "* development time is faster than using any of the compiled language \n",
      "( other than probably byte-compiled python for me)\n",
      "* one of the shortest implementation in terms of lines of code\n",
      "* no fancy algorithms are needed \n",
      "* good for prototyping which is what usually people do in industry\n",
      "\n",
      "Cons:\n",
      "\n",
      "* runtime will be slower than using compiled language for doing string manipulation \n",
      "* the computation of the frequency table is not the most parallelizble\n",
      "* not a good production code due to the above cons\n",
      "\n",
      "Other thoughts about implementation:\n",
      "\n",
      "Not going to write the code in the most general / extensible / parallelizable way but I focus on getting an estimate of the answer from this first method\n",
      "\n",
      "I usually write code as functions and use OOP concepts but I don't have time for this.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.1 Shell scripting part"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observation: there are two types of files with two different headers. \n",
      "\n",
      "* month-by-month csv  \n",
      "* year-by-year csv\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "extracting columns from the csv files is quite straight forward except that, in the month-by-month csv, there are some values with \",\" in them, making the naive use of cut with \",\" as delimiter fail to fetch the correct column. My quick and dirty solution is to increment the column number count by 2 as a correction.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Manual testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See 'exam_col.sh' and 'test.ipynb' : \n",
      "I make sure that the functions work on individual files before looping them over more files.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Other possible tests that I do not have time for: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* output all the headers of the year-by-year csv and month-by-month csv and make sure there are no variety within each type of csv\n",
      "\n",
      "Also, on second thought I should have written a test code, using python or R to write out mean, median and standard deviation of every csv one by one.\n",
      "\n",
      "Then also use my code to write output of same format.\n",
      "Then just use diff in the shell to check if there is any differences to check for as many possible exceptional cases as possible.\n",
      "\n",
      "Now I'm just manually comparing statistics of individual files / writing out fake data for testing.\n",
      "\n",
      "I really should have done a better job planning for unit tests ......"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. R part - computing the statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When I write in a scripting language, I usually \n",
      "\n",
      "* keep code short \n",
      "* avoid loops and use vectorized operations \n",
      "\n",
      "Since R is not particular strong in string manipulation, I try to keep all string operations with the shell script."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Considerations for preventing numerical inaccuracies with the frequency table approach:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get a sense of the order of magnitude of the numbers that we are computing : \n",
      "\n",
      "* total unique frequency count is $\\omega_{total} \\sim 10^8$\n",
      "* range of the delay time  [-1437, 2598]\n",
      "* all delay time are integers which simplifies that problem by a lot \n",
      "* precision requirement is lax (up to one minuate)\n",
      "\n",
      "Of course these stats are obtained after I have run them through my scripts.\n",
      "On second thought I should have find/guesstimate the properties of the data before deciding on what languages / tools to use for this homework.\n",
      "\n",
      "For R, \n",
      "\n",
      "* the maximum value that an integer can hold is $\\sim 10^9$, and \n",
      "* double can hold much larger integer $\\sim 10^{308}$ and is the default R data type for numbers\n",
      "* Machine precision for double is $\\sim 10^{-16}$. \n",
      "\n",
      "Since R uses double for storing the ArrDelay field by default and data take integer form, we really shouldn't have to worry too much about numerical inaccuracies like overflow (when summing number), underflow (dividing by large number) or cancellation error (subtraction). I argue we do not need more rigorous algorithms in the world for just computing the statistics of ArrDelay with the frequency table approach. If we compute the statistics with other methods, we may have to worry more."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Computing the mean:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "danger of overflow of this naive formula:\n",
      "\n",
      "$$ \\bar{t} = \\frac{ \\sum_i^n (t_i ~\\omega_i)}{\\omega_{total}}$$\n",
      "\n",
      "where  $t_i$ denotes the delay time $\\omega_i $ denotes the corresponding count for that $t_i$.\n",
      "Numerator may overflow if we are summing a bunch of numbers that are too large\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a safety precaution I will compute the mean using \n",
      "\n",
      "$$ \\bar{t} =  \\sum_i^n \\frac{(t_i ~\\omega_i)}{\\omega_{total}}$$\n",
      "\n",
      "Note $t_i, w_i \\ll w_{total}$, by multiplying before dividing, there shouldn't be any underflow problems."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Computing the std. dev.:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After reading John Cook 's entry I decide to use an adapted version of the two-pass formula because I can write vectorized code instead of looping in R\n",
      "\n",
      "$$s = \\sqrt{(t} $$  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Computing the median:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have to be careful if total frequency count is odd or even or else might grab the wrong median."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Results:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "......eyeballing if answer make sense by plotting the frequency counts\n",
      "\n",
      "the shape of the histogram makes sense."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Computation time:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is probably not the fastest approach in terms of CPU time. \n",
      "\n",
      "I have coded up two different versions of the bash script. \n",
      "First version collect the columns from different csv and write them all out to a big file before sorting then counting frequencies. \n",
      "I expect the I/O to be one of the bigger bottlenecks.\n",
      "\n",
      "Second version tries to create an array to hold all the relevant columns then perform sorting and dropping duplicates. The second approach avoids I/O but may use up more memory due to my lack of good way of growing arrays.\n",
      "\n",
      "I could also do the "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CPU time: \n",
      "\n",
      "It is faster than I can take a bathroom break."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Memory usage: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No swap was used as far as I am concerned"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Machine specification:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Corsair Vengeance 2x8GB DDR3 1600 MHz Desktop Memory, \n",
      "* Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz, http://ark.intel.com/products/75123\n",
      "* GeForce GTX 770 SuperClocked\n",
      "* Samsung 840 Pro 256 GB SSD\n",
      "* Motherboard: Asus Z87-Deluxe DDR3 1600 LGA 1150 \n",
      "* Linux Mint 15 Olivia (GNU/Linux 3.8.0-19-generic x86_64)\n",
      "* R version 3.0.2 (2013-09-25) -- \"Frisbee Sailing\"\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bonus question:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How did Duncan download the data off of the following website?\n",
      "http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ans (trying to gain some extra brownie points):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Short answer: download & start reading from P.333 of  http://link.springer.com/book/10.1007%2F978-1-4614-7900-0\n",
      "\n",
      "Long answer: I could guesstimate how to fill in the dynamic form on that website with some scripts and pull the links to those forms for downloading the data. But I will just get back to working on the other parts of the homework... "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Comparison of methods: \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exact Method 2: building a database "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exact Method 3: byte compiled python"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Acknowledgement(s) / reference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thanks to an anonymous physicist in Portland who has kindly allowed me to use his gaming desktop for the computing of this homework. He helped me repair the partition table to my Linux partition and installed a HDD for use."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}